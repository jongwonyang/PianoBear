{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 18:13:59.594993: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-21 18:13:59.739223: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-21 18:13:59.739247: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-21 18:13:59.741074: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-21 18:13:59.794222: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce GTX 1660 SUPER, compute capability 7.5\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce GTX 1660 SUPER, compute capability 7.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 18:14:02.331514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-21 18:14:02.348920: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-21 18:14:02.348949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-21 18:14:02.349148: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-21 18:14:02.462334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-21 18:14:02.467679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-21 18:14:02.467763: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-21 18:14:02.467775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-21 18:14:03.396365: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-21 18:14:03.396406: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-21 18:14:03.396412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-07-21 18:14:03.396433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-21 18:14:03.396477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3914 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-07-21 18:14:03.673584: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 18:14:08.721687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 2s 19ms/step\n",
      "22/22 [==============================] - 2s 18ms/step\n",
      "saving...\n",
      "save complete\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from os.path import split, join\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "from pretty_midi import Note\n",
    "from pydub import AudioSegment\n",
    "from tensorflow import keras\n",
    "import tensorflow.python.keras.mixed_precision.policy as mixed_precision\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "AudioSegment.converter = 'ffmpeg'\n",
    "\n",
    "def one_to_midi(notes, offsets, file_name, time_per_frame):\n",
    "    pm = pretty_midi.PrettyMIDI()\n",
    "    instrument = pretty_midi.Instrument(program=1)\n",
    "\n",
    "    notes = notes.T\n",
    "    offsets = offsets.T\n",
    "    for pitch, hor in enumerate(notes):\n",
    "        nz = np.where(hor != 0)[0]\n",
    "        if len(nz) == 0:\n",
    "            continue\n",
    "\n",
    "        visit = np.zeros_like(hor, dtype=bool)\n",
    "        off = offsets[pitch]\n",
    "        for idx in nz:\n",
    "            i = idx\n",
    "            while i < len(off) and off[i] != 0:\n",
    "                visit[i] = True\n",
    "                i += 1\n",
    "\n",
    "        idx = 0\n",
    "        while idx < len(visit):\n",
    "            start_time = idx * time_per_frame\n",
    "            end_time = start_time\n",
    "\n",
    "            while idx < len(visit) and visit[idx]:\n",
    "                end_time += time_per_frame\n",
    "                idx += 1\n",
    "\n",
    "            if start_time != end_time:\n",
    "                instrument.notes.append(Note(\n",
    "                    velocity=100, pitch=pitch + 21, start=start_time, end=end_time))\n",
    "            idx += 1\n",
    "\n",
    "    print('saving...')\n",
    "    pm.instruments.append(instrument)\n",
    "    pm.write(file_name)\n",
    "    print('save complete')\n",
    "\n",
    "def preprocess_audio(y, sr, one_seq, batch_size):\n",
    "    cqt = np.abs(librosa.cqt(y, sr=sr, fmin=librosa.midi_to_hz(21), n_bins=264, hop_length=160, bins_per_octave=36))\n",
    "    stft = librosa.stft(y, n_fft=512, hop_length=160)\n",
    "\n",
    "    cqt = cqt.T / np.max(cqt)\n",
    "    stft = stft.T / np.max(stft)\n",
    "\n",
    "    pad_size = one_seq - (cqt.shape[0] % one_seq)\n",
    "    cqt = np.pad(cqt, ((0, pad_size), (0, 0)), mode='constant')\n",
    "    stft = np.pad(stft, ((0, pad_size), (0, 0)), mode='constant')\n",
    "\n",
    "    cqts = cqt.reshape(cqt.shape[0] // one_seq, one_seq, 264)\n",
    "    stfts = stft.reshape(stft.shape[0] // one_seq, one_seq, 257)\n",
    "\n",
    "    desired_cqt_shape = (batch_size * ((cqts.shape[0] + batch_size - 1) // batch_size), one_seq, 264)\n",
    "    desired_stft_shape = (batch_size * ((stfts.shape[0] + batch_size - 1) // batch_size), one_seq, 257)\n",
    "\n",
    "    cqt_padding_shape = (desired_cqt_shape[0] - cqts.shape[0], 0, 0)\n",
    "    stft_padding_shape = (desired_stft_shape[0] - stfts.shape[0], 0, 0)\n",
    "\n",
    "    cqts = np.pad(cqts, ((0, cqt_padding_shape[0]), (0, cqt_padding_shape[1]), (0, cqt_padding_shape[2])), mode='constant')\n",
    "    stfts = np.pad(stfts, ((0, stft_padding_shape[0]), (0, stft_padding_shape[1]), (0, stft_padding_shape[2])), mode='constant')\n",
    "\n",
    "    return {'cqt_input': cqts, 'stft_input': stfts}\n",
    "\n",
    "def predict_notes(len_model, onset_model, preprocessed, one_seq, batch_size):\n",
    "    len_result = len_model.predict(preprocessed, batch_size=batch_size)\n",
    "    onset_result = onset_model.predict(preprocessed, batch_size=batch_size)\n",
    "\n",
    "    onset = onset_result.reshape(-1, 88)\n",
    "    offset = len_result.reshape(-1, 88)\n",
    "\n",
    "    onset = (onset >= 0.5).astype(int)\n",
    "    offset = (offset >= 0.3).astype(int)\n",
    "\n",
    "    return onset, offset\n",
    "\n",
    "def test(X_test_path):\n",
    "    len_model = keras.models.load_model(\"models/offset_detector_v2.h5\")\n",
    "    onset_model = keras.models.load_model(\"models/onset_detector_v2.h5\")\n",
    "    print('model loaded')\n",
    "\n",
    "    y, sr = librosa.load(X_test_path, sr=16000)\n",
    "    one_seq = 100\n",
    "    batch_size = 10\n",
    "\n",
    "    preprocessed = preprocess_audio(y, sr, one_seq, batch_size)\n",
    "    onset, offset = predict_notes(len_model, onset_model, preprocessed, one_seq, batch_size)\n",
    "\n",
    "    time_per_frame = librosa.frames_to_time(1, sr=sr, hop_length=160)\n",
    "    midi_file_path = join('data/', split(X_test_path)[-1][:-4] + '.mid')\n",
    "    one_to_midi(notes=onset, offsets=offset, file_name=midi_file_path, time_per_frame=time_per_frame)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test('data/tetris.mp3')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
