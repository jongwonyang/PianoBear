{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 02:49:19.512995: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-21 02:49:19.716906: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-21 02:49:19.716928: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-21 02:49:19.718012: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-21 02:49:19.804790: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 02:49:22.005525: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-21 02:49:22.022717: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-21 02:49:22.022750: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onset 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Dense, Bidirectional, LSTM, TimeDistributed, Concatenate\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.mixed_precision import Policy, set_global_policy\n",
    "\n",
    "# 혼합 정밀도 정책 설정\n",
    "policy = Policy('mixed_float16')\n",
    "set_global_policy(policy)\n",
    "\n",
    "def data_generator(cqt_paths, stft_paths, onset_paths, model):\n",
    "    for cqt_path, stft_path, onset_path in zip(cqt_paths, stft_paths, onset_paths):\n",
    "        CQT = np.load(cqt_path)\n",
    "        STFT = np.load(stft_path)\n",
    "        ONSET = np.load(onset_path)\n",
    "\n",
    "        # 정규화\n",
    "        CQT = CQT / np.max(CQT)\n",
    "        STFT = STFT / np.max(STFT)\n",
    "\n",
    "        model.reset_states()  # 한 곡마다 모델 상태 초기화\n",
    "        for cqt, stft, onset in zip(CQT, STFT, ONSET):\n",
    "            yield ({'cqt_input': cqt, 'stft_input': stft}, onset)  # 한 세그먼트씩 반환\n",
    "\n",
    "def build_model():\n",
    "    cqt_input = Input(batch_input_shape=(10, 100, 264), name='cqt_input')\n",
    "    stft_input = Input(batch_input_shape=(10, 100, 257), name='stft_input')\n",
    "\n",
    "    cqt_lstm = Bidirectional(LSTM(128, activation='tanh', return_sequences=True, stateful=True))(cqt_input)\n",
    "    stft_lstm = Bidirectional(LSTM(128, activation='tanh', return_sequences=True, stateful=True))(stft_input)\n",
    "\n",
    "    concatenated = Concatenate()([cqt_lstm, stft_lstm])\n",
    "\n",
    "    onset_out = TimeDistributed(Dense(88, activation='sigmoid', kernel_initializer='he_normal', name='onset_output'))(concatenated)\n",
    "    model = Model(inputs=[cqt_input, stft_input], outputs=onset_out)\n",
    "\n",
    "    return model\n",
    "\n",
    "def train(train_cqt, train_stft, train_onset, valid_cqt, valid_stft, valid_onset):\n",
    "    train_cqt_paths, train_stft_paths, train_onset_paths = [], [], []\n",
    "    valid_cqt_paths, valid_stft_paths, valid_onset_paths = [], [], []\n",
    "\n",
    "    for cqt_path, stft_path, onset_path in zip(glob.glob(train_cqt), glob.glob(train_stft), glob.glob(train_onset)):\n",
    "        train_cqt_paths.append(cqt_path)\n",
    "        train_stft_paths.append(stft_path)\n",
    "        train_onset_paths.append(onset_path)\n",
    "\n",
    "    for cqt_path, stft_path, onset_path in zip(glob.glob(valid_cqt), glob.glob(valid_stft), glob.glob(valid_onset)):\n",
    "        valid_cqt_paths.append(cqt_path)\n",
    "        valid_stft_paths.append(stft_path)\n",
    "        valid_onset_paths.append(onset_path)\n",
    "\n",
    "    output_signature = (\n",
    "        {\n",
    "            'cqt_input': tf.TensorSpec(shape=(100, 264), dtype=tf.float32),\n",
    "            'stft_input': tf.TensorSpec(shape=(100, 257), dtype=tf.float32)\n",
    "        },\n",
    "        tf.TensorSpec(shape=(100, 88), dtype=tf.int8)\n",
    "    )\n",
    "\n",
    "    model = build_model()\n",
    "\n",
    "    train_set = tf.data.Dataset.from_generator(lambda: data_generator(train_cqt_paths, train_stft_paths, train_onset_paths, model), output_signature=output_signature)\n",
    "    valid_set = tf.data.Dataset.from_generator(lambda: data_generator(valid_cqt_paths, valid_stft_paths, valid_onset_paths, model), output_signature=output_signature)\n",
    "    train_set = train_set.batch(10, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    valid_set = valid_set.batch(10, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    checkpoint = ModelCheckpoint('models/onset_detector_v2.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "    early_stop = EarlyStopping(patience=5, monitor='val_loss', verbose=1, mode='auto')\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    model.fit(train_set, validation_data=valid_set, epochs=2, shuffle=False, callbacks=[checkpoint, early_stop])\n",
    "    model.save('models/onset_last_v2.h5')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train(\n",
    "        'data/preprocessed/trainX/cqt/*.npy',\n",
    "        'data/preprocessed/trainX/stft/*.npy',\n",
    "        'data/preprocessed/trainONSET/*.npy',\n",
    "        'data/preprocessed/validX/cqt/*.npy',\n",
    "        'data/preprocessed/validX/stft/*.npy',\n",
    "        'data/preprocessed/validONSET/*.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offset 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Dense, Bidirectional, LSTM, TimeDistributed, Concatenate\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.mixed_precision import Policy, set_global_policy\n",
    "\n",
    "# 혼합 정밀도 정책 설정\n",
    "policy = Policy('mixed_float16')\n",
    "set_global_policy(policy)\n",
    "\n",
    "def data_generator(cqt_paths, stft_paths, offset_paths, model):\n",
    "    for cqt_path, stft_path, offset_path in zip(cqt_paths, stft_paths, offset_paths):\n",
    "        CQT = np.load(cqt_path)\n",
    "        STFT = np.load(stft_path)\n",
    "        OFFSET = np.load(offset_path)\n",
    "\n",
    "        # 정규화\n",
    "        CQT = CQT / np.max(CQT)\n",
    "        STFT = STFT / np.max(STFT)\n",
    "\n",
    "        model.reset_states()  # 한 곡마다 모델 상태 초기화\n",
    "        for cqt, stft, offset in zip(CQT, STFT, OFFSET):\n",
    "            yield ({'cqt_input': cqt, 'stft_input': stft}, offset)  # 한 세그먼트씩 반환\n",
    "\n",
    "def build_model():\n",
    "    cqt_input = Input(batch_input_shape=(10, 100, 264), name='cqt_input')\n",
    "    stft_input = Input(batch_input_shape=(10, 100, 257), name='stft_input')\n",
    "\n",
    "    cqt_lstm = Bidirectional(LSTM(128, activation='tanh', return_sequences=True, stateful=True))(cqt_input)\n",
    "    stft_lstm = Bidirectional(LSTM(128, activation='tanh', return_sequences=True, stateful=True))(stft_input)\n",
    "\n",
    "    concatenated = Concatenate()([cqt_lstm, stft_lstm])\n",
    "\n",
    "    offset_out = TimeDistributed(Dense(88, activation='sigmoid', kernel_initializer='he_normal', name='offset_output'))(concatenated)\n",
    "    model = Model(inputs=[cqt_input, stft_input], outputs=offset_out)\n",
    "\n",
    "    return model\n",
    "\n",
    "def train(train_cqt, train_stft, train_offset, valid_cqt, valid_stft, valid_offset):\n",
    "    train_cqt_paths, train_stft_paths, train_offset_paths = [], [], []\n",
    "    valid_cqt_paths, valid_stft_paths, valid_offset_paths = [], [], []\n",
    "\n",
    "    for cqt_path, stft_path, offset_path in zip(glob.glob(train_cqt), glob.glob(train_stft), glob.glob(train_offset)):\n",
    "        train_cqt_paths.append(cqt_path)\n",
    "        train_stft_paths.append(stft_path)\n",
    "        train_offset_paths.append(offset_path)\n",
    "\n",
    "    for cqt_path, stft_path, offset_path in zip(glob.glob(valid_cqt), glob.glob(valid_stft), glob.glob(valid_offset)):\n",
    "        valid_cqt_paths.append(cqt_path)\n",
    "        valid_stft_paths.append(stft_path)\n",
    "        valid_offset_paths.append(offset_path)\n",
    "\n",
    "    output_signature = (\n",
    "        {\n",
    "            'cqt_input': tf.TensorSpec(shape=(100, 264), dtype=tf.float32),\n",
    "            'stft_input': tf.TensorSpec(shape=(100, 257), dtype=tf.float32)\n",
    "        },\n",
    "        tf.TensorSpec(shape=(100, 88), dtype=tf.int8)\n",
    "    )\n",
    "\n",
    "    model = build_model()\n",
    "\n",
    "    train_set = tf.data.Dataset.from_generator(lambda: data_generator(train_cqt_paths, train_stft_paths, train_offset_paths, model), output_signature=output_signature)\n",
    "    valid_set = tf.data.Dataset.from_generator(lambda: data_generator(valid_cqt_paths, valid_stft_paths, valid_offset_paths, model), output_signature=output_signature)\n",
    "    train_set = train_set.batch(10, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    valid_set = valid_set.batch(10, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    checkpoint = ModelCheckpoint('models/offset_detector_v2.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "    early_stop = EarlyStopping(patience=5, monitor='val_loss', verbose=1, mode='auto')\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    model.fit(train_set, validation_data=valid_set, epochs=2, shuffle=False, callbacks=[checkpoint, early_stop])\n",
    "    model.save('models/offset_last_v2.h5')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train(\n",
    "        'data/preprocessed/trainX/cqt/*.npy',\n",
    "        'data/preprocessed/trainX/stft/*.npy',\n",
    "        'data/preprocessed/trainOFFSET/*.npy',\n",
    "        'data/preprocessed/validX/cqt/*.npy',\n",
    "        'data/preprocessed/validX/stft/*.npy',\n",
    "        'data/preprocessed/validOFFSET/*.npy')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
